Vendor:  Continuum Analytics, Inc.
Package: mkl
Message: trial mode expires in 10 days
Vendor:  Continuum Analytics, Inc.
Package: mkl
Message: trial mode expires in 10 days
WARNING: Logging before InitGoogleLogging() is written to STDERR
I0302 01:27:40.205600 29253 solver.cpp:48] Initializing solver from parameters: 
base_lr: 0.01
display: 20
max_iter: 100000
lr_policy: "step"
gamma: 0.1
momentum: 0.9
weight_decay: 0.0005
stepsize: 10000
snapshot: 5000
snapshot_prefix: "/nfs.yoda/xiaolonw/fast_rcnn/models_sunrgbd/pre_cls_1fc_scratch/model_"
solver_mode: GPU
net: "train.prototxt"
I0302 01:27:40.207401 29253 solver.cpp:91] Creating training net from net file: train.prototxt
I0302 01:27:40.208941 29253 net.cpp:49] Initializing net from parameters: 
name: "sungrbd"
state {
  phase: TRAIN
}
layer {
  name: "data"
  type: "ImageData"
  top: "data"
  top: "label"
  include {
    phase: TRAIN
  }
  transform_param {
    mirror: false
    crop_size: 128
    mean_value: 1
  }
  image_data_param {
    source: "/nfs/hn38/users/xiaolonw/sunrgbd/SUNRGBDtoolbox/trainlist2.txt"
    batch_size: 100
    shuffle: true
    new_height: 128
    new_width: 128
    root_folder: "/scratch/xiaolonw/sunrgbd/data/"
  }
}
layer {
  name: "da_conv1"
  type: "Convolution"
  bottom: "data"
  top: "da_conv1"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 64
    pad: 2
    kernel_size: 5
    stride: 2
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "bn1"
  type: "BatchNorm"
  bottom: "da_conv1"
  top: "bn1"
}
layer {
  name: "da_relu1"
  type: "ReLU"
  bottom: "bn1"
  top: "bn1"
  relu_param {
    negative_slope: 0.2
  }
}
layer {
  name: "da_conv2"
  type: "Convolution"
  bottom: "bn1"
  top: "da_conv2"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 128
    pad: 2
    kernel_size: 5
    stride: 2
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 1
    }
  }
}
layer {
  name: "bn2"
  type: "BatchNorm"
  bottom: "da_conv2"
  top: "bn2"
}
layer {
  name: "da_relu2"
  type: "ReLU"
  bottom: "bn2"
  top: "bn2"
  relu_param {
    negative_slope: 0.2
  }
}
layer {
  name: "da_conv3"
  type: "Convolution"
  bottom: "bn2"
  top: "da_conv3"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 256
    pad: 1
    kernel_size: 3
    stride: 2
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "bn3"
  type: "BatchNorm"
  bottom: "da_conv3"
  top: "bn3"
}
layer {
  name: "da_relu3"
  type: "ReLU"
  bottom: "bn3"
  top: "bn3"
  relu_param {
    negative_slope: 0.2
  }
}
layer {
  name: "da_conv4"
  type: "Convolution"
  bottom: "bn3"
  top: "da_conv4"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 512
    pad: 1
    kernel_size: 3
    stride: 2
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 1
    }
  }
}
layer {
  name: "bn4"
  type: "BatchNorm"
  bottom: "da_conv4"
  top: "bn4"
}
layer {
  name: "da_relu4"
  type: "ReLU"
  bottom: "bn4"
  top: "bn4"
  relu_param {
    negative_slope: 0.2
  }
}
layer {
  name: "da_conv5"
  type: "Convolution"
  bottom: "bn4"
  top: "da_conv5"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 128
    pad: 1
    kernel_size: 3
    stride: 1
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 1
    }
  }
}
layer {
  name: "bn5"
  type: "BatchNorm"
  bottom: "da_conv5"
  top: "bn5"
}
layer {
  name: "da_relu5"
  type: "ReLU"
  bottom: "bn5"
  top: "bn5"
  relu_param {
    negative_slope: 0.2
  }
}
layer {
  name: "pool5"
  type: "Pooling"
  bottom: "bn5"
  top: "pool5"
  pooling_param {
    pool: MAX
    kernel_size: 3
    stride: 2
  }
}
layer {
  name: "cls_score"
  type: "InnerProduct"
  bottom: "pool5"
  top: "cls_score"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  inner_product_param {
    num_output: 19
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "loss_cls"
  type: "SoftmaxWithLoss"
  bottom: "cls_score"
  bottom: "label"
  top: "loss_cls"
}
I0302 01:27:40.209038 29253 layer_factory.hpp:77] Creating layer data
I0302 01:27:40.209059 29253 net.cpp:106] Creating Layer data
I0302 01:27:40.209065 29253 net.cpp:411] data -> data
I0302 01:27:40.209081 29253 net.cpp:411] data -> label
I0302 01:27:40.209682 29253 image_data_layer.cpp:38] Opening file /nfs/hn38/users/xiaolonw/sunrgbd/SUNRGBDtoolbox/trainlist2.txt
I0302 01:27:40.281720 29253 image_data_layer.cpp:51] Shuffling data
I0302 01:27:40.282872 29253 image_data_layer.cpp:56] A total of 4845 images.
I0302 01:27:40.297765 29253 image_data_layer.cpp:84] output data size: 100,6,128,128
I0302 01:27:40.359657 29253 net.cpp:150] Setting up data
I0302 01:27:40.359699 29253 net.cpp:157] Top shape: 100 6 128 128 (9830400)
I0302 01:27:40.359707 29253 net.cpp:157] Top shape: 100 (100)
I0302 01:27:40.359710 29253 net.cpp:165] Memory required for data: 39322000
I0302 01:27:40.359719 29253 layer_factory.hpp:77] Creating layer da_conv1
I0302 01:27:40.359741 29253 net.cpp:106] Creating Layer da_conv1
I0302 01:27:40.359746 29253 net.cpp:454] da_conv1 <- data
I0302 01:27:40.359755 29253 net.cpp:411] da_conv1 -> da_conv1
I0302 01:27:40.361129 29253 net.cpp:150] Setting up da_conv1
I0302 01:27:40.361142 29253 net.cpp:157] Top shape: 100 64 64 64 (26214400)
I0302 01:27:40.361146 29253 net.cpp:165] Memory required for data: 144179600
I0302 01:27:40.361160 29253 layer_factory.hpp:77] Creating layer bn1
I0302 01:27:40.361169 29253 net.cpp:106] Creating Layer bn1
I0302 01:27:40.361174 29253 net.cpp:454] bn1 <- da_conv1
I0302 01:27:40.361181 29253 net.cpp:411] bn1 -> bn1
I0302 01:27:40.361408 29253 net.cpp:150] Setting up bn1
I0302 01:27:40.361418 29253 net.cpp:157] Top shape: 100 64 64 64 (26214400)
I0302 01:27:40.361420 29253 net.cpp:165] Memory required for data: 249037200
I0302 01:27:40.361435 29253 layer_factory.hpp:77] Creating layer da_relu1
I0302 01:27:40.361443 29253 net.cpp:106] Creating Layer da_relu1
I0302 01:27:40.361448 29253 net.cpp:454] da_relu1 <- bn1
I0302 01:27:40.361452 29253 net.cpp:397] da_relu1 -> bn1 (in-place)
I0302 01:27:40.361465 29253 net.cpp:150] Setting up da_relu1
I0302 01:27:40.361470 29253 net.cpp:157] Top shape: 100 64 64 64 (26214400)
I0302 01:27:40.361474 29253 net.cpp:165] Memory required for data: 353894800
I0302 01:27:40.361479 29253 layer_factory.hpp:77] Creating layer da_conv2
I0302 01:27:40.361488 29253 net.cpp:106] Creating Layer da_conv2
I0302 01:27:40.361492 29253 net.cpp:454] da_conv2 <- bn1
I0302 01:27:40.361498 29253 net.cpp:411] da_conv2 -> da_conv2
I0302 01:27:40.365769 29253 net.cpp:150] Setting up da_conv2
I0302 01:27:40.365783 29253 net.cpp:157] Top shape: 100 128 32 32 (13107200)
I0302 01:27:40.365787 29253 net.cpp:165] Memory required for data: 406323600
I0302 01:27:40.365793 29253 layer_factory.hpp:77] Creating layer bn2
I0302 01:27:40.365800 29253 net.cpp:106] Creating Layer bn2
I0302 01:27:40.365803 29253 net.cpp:454] bn2 <- da_conv2
I0302 01:27:40.365808 29253 net.cpp:411] bn2 -> bn2
I0302 01:27:40.365994 29253 net.cpp:150] Setting up bn2
I0302 01:27:40.366003 29253 net.cpp:157] Top shape: 100 128 32 32 (13107200)
I0302 01:27:40.366006 29253 net.cpp:165] Memory required for data: 458752400
I0302 01:27:40.366017 29253 layer_factory.hpp:77] Creating layer da_relu2
I0302 01:27:40.366025 29253 net.cpp:106] Creating Layer da_relu2
I0302 01:27:40.366030 29253 net.cpp:454] da_relu2 <- bn2
I0302 01:27:40.366034 29253 net.cpp:397] da_relu2 -> bn2 (in-place)
I0302 01:27:40.366042 29253 net.cpp:150] Setting up da_relu2
I0302 01:27:40.366046 29253 net.cpp:157] Top shape: 100 128 32 32 (13107200)
I0302 01:27:40.366050 29253 net.cpp:165] Memory required for data: 511181200
I0302 01:27:40.366053 29253 layer_factory.hpp:77] Creating layer da_conv3
I0302 01:27:40.366060 29253 net.cpp:106] Creating Layer da_conv3
I0302 01:27:40.366065 29253 net.cpp:454] da_conv3 <- bn2
I0302 01:27:40.366070 29253 net.cpp:411] da_conv3 -> da_conv3
I0302 01:27:40.372233 29253 net.cpp:150] Setting up da_conv3
I0302 01:27:40.372248 29253 net.cpp:157] Top shape: 100 256 16 16 (6553600)
I0302 01:27:40.372251 29253 net.cpp:165] Memory required for data: 537395600
I0302 01:27:40.372259 29253 layer_factory.hpp:77] Creating layer bn3
I0302 01:27:40.372267 29253 net.cpp:106] Creating Layer bn3
I0302 01:27:40.372272 29253 net.cpp:454] bn3 <- da_conv3
I0302 01:27:40.372277 29253 net.cpp:411] bn3 -> bn3
I0302 01:27:40.372454 29253 net.cpp:150] Setting up bn3
I0302 01:27:40.372462 29253 net.cpp:157] Top shape: 100 256 16 16 (6553600)
I0302 01:27:40.372467 29253 net.cpp:165] Memory required for data: 563610000
I0302 01:27:40.372474 29253 layer_factory.hpp:77] Creating layer da_relu3
I0302 01:27:40.372483 29253 net.cpp:106] Creating Layer da_relu3
I0302 01:27:40.372488 29253 net.cpp:454] da_relu3 <- bn3
I0302 01:27:40.372493 29253 net.cpp:397] da_relu3 -> bn3 (in-place)
I0302 01:27:40.372498 29253 net.cpp:150] Setting up da_relu3
I0302 01:27:40.372503 29253 net.cpp:157] Top shape: 100 256 16 16 (6553600)
I0302 01:27:40.372505 29253 net.cpp:165] Memory required for data: 589824400
I0302 01:27:40.372509 29253 layer_factory.hpp:77] Creating layer da_conv4
I0302 01:27:40.372515 29253 net.cpp:106] Creating Layer da_conv4
I0302 01:27:40.372519 29253 net.cpp:454] da_conv4 <- bn3
I0302 01:27:40.372524 29253 net.cpp:411] da_conv4 -> da_conv4
I0302 01:27:40.392678 29253 net.cpp:150] Setting up da_conv4
I0302 01:27:40.392695 29253 net.cpp:157] Top shape: 100 512 8 8 (3276800)
I0302 01:27:40.392699 29253 net.cpp:165] Memory required for data: 602931600
I0302 01:27:40.392710 29253 layer_factory.hpp:77] Creating layer bn4
I0302 01:27:40.392719 29253 net.cpp:106] Creating Layer bn4
I0302 01:27:40.392724 29253 net.cpp:454] bn4 <- da_conv4
I0302 01:27:40.392732 29253 net.cpp:411] bn4 -> bn4
I0302 01:27:40.392920 29253 net.cpp:150] Setting up bn4
I0302 01:27:40.392928 29253 net.cpp:157] Top shape: 100 512 8 8 (3276800)
I0302 01:27:40.392932 29253 net.cpp:165] Memory required for data: 616038800
I0302 01:27:40.392941 29253 layer_factory.hpp:77] Creating layer da_relu4
I0302 01:27:40.392948 29253 net.cpp:106] Creating Layer da_relu4
I0302 01:27:40.392953 29253 net.cpp:454] da_relu4 <- bn4
I0302 01:27:40.392957 29253 net.cpp:397] da_relu4 -> bn4 (in-place)
I0302 01:27:40.392966 29253 net.cpp:150] Setting up da_relu4
I0302 01:27:40.392971 29253 net.cpp:157] Top shape: 100 512 8 8 (3276800)
I0302 01:27:40.392973 29253 net.cpp:165] Memory required for data: 629146000
I0302 01:27:40.392976 29253 layer_factory.hpp:77] Creating layer da_conv5
I0302 01:27:40.392983 29253 net.cpp:106] Creating Layer da_conv5
I0302 01:27:40.392987 29253 net.cpp:454] da_conv5 <- bn4
I0302 01:27:40.392994 29253 net.cpp:411] da_conv5 -> da_conv5
I0302 01:27:40.403224 29253 net.cpp:150] Setting up da_conv5
I0302 01:27:40.403237 29253 net.cpp:157] Top shape: 100 128 8 8 (819200)
I0302 01:27:40.403240 29253 net.cpp:165] Memory required for data: 632422800
I0302 01:27:40.403247 29253 layer_factory.hpp:77] Creating layer bn5
I0302 01:27:40.403255 29253 net.cpp:106] Creating Layer bn5
I0302 01:27:40.403259 29253 net.cpp:454] bn5 <- da_conv5
I0302 01:27:40.403264 29253 net.cpp:411] bn5 -> bn5
I0302 01:27:40.403444 29253 net.cpp:150] Setting up bn5
I0302 01:27:40.403450 29253 net.cpp:157] Top shape: 100 128 8 8 (819200)
I0302 01:27:40.403455 29253 net.cpp:165] Memory required for data: 635699600
I0302 01:27:40.403463 29253 layer_factory.hpp:77] Creating layer da_relu5
I0302 01:27:40.403470 29253 net.cpp:106] Creating Layer da_relu5
I0302 01:27:40.403475 29253 net.cpp:454] da_relu5 <- bn5
I0302 01:27:40.403480 29253 net.cpp:397] da_relu5 -> bn5 (in-place)
I0302 01:27:40.403486 29253 net.cpp:150] Setting up da_relu5
I0302 01:27:40.403491 29253 net.cpp:157] Top shape: 100 128 8 8 (819200)
I0302 01:27:40.403493 29253 net.cpp:165] Memory required for data: 638976400
I0302 01:27:40.403496 29253 layer_factory.hpp:77] Creating layer pool5
I0302 01:27:40.403504 29253 net.cpp:106] Creating Layer pool5
I0302 01:27:40.403507 29253 net.cpp:454] pool5 <- bn5
I0302 01:27:40.403512 29253 net.cpp:411] pool5 -> pool5
I0302 01:27:40.403553 29253 net.cpp:150] Setting up pool5
I0302 01:27:40.403559 29253 net.cpp:157] Top shape: 100 128 4 4 (204800)
I0302 01:27:40.403563 29253 net.cpp:165] Memory required for data: 639795600
I0302 01:27:40.403565 29253 layer_factory.hpp:77] Creating layer cls_score
I0302 01:27:40.403578 29253 net.cpp:106] Creating Layer cls_score
I0302 01:27:40.403583 29253 net.cpp:454] cls_score <- pool5
I0302 01:27:40.403587 29253 net.cpp:411] cls_score -> cls_score
I0302 01:27:40.404788 29253 net.cpp:150] Setting up cls_score
I0302 01:27:40.404800 29253 net.cpp:157] Top shape: 100 19 (1900)
I0302 01:27:40.404803 29253 net.cpp:165] Memory required for data: 639803200
I0302 01:27:40.404811 29253 layer_factory.hpp:77] Creating layer loss_cls
I0302 01:27:40.404819 29253 net.cpp:106] Creating Layer loss_cls
I0302 01:27:40.404824 29253 net.cpp:454] loss_cls <- cls_score
I0302 01:27:40.404829 29253 net.cpp:454] loss_cls <- label
I0302 01:27:40.404834 29253 net.cpp:411] loss_cls -> loss_cls
I0302 01:27:40.404847 29253 layer_factory.hpp:77] Creating layer loss_cls
I0302 01:27:40.404937 29253 net.cpp:150] Setting up loss_cls
I0302 01:27:40.404943 29253 net.cpp:157] Top shape: (1)
I0302 01:27:40.404947 29253 net.cpp:160]     with loss weight 1
I0302 01:27:40.404955 29253 net.cpp:165] Memory required for data: 639803204
I0302 01:27:40.404959 29253 net.cpp:226] loss_cls needs backward computation.
I0302 01:27:40.404963 29253 net.cpp:226] cls_score needs backward computation.
I0302 01:27:40.404968 29253 net.cpp:226] pool5 needs backward computation.
I0302 01:27:40.404970 29253 net.cpp:226] da_relu5 needs backward computation.
I0302 01:27:40.404974 29253 net.cpp:226] bn5 needs backward computation.
I0302 01:27:40.404978 29253 net.cpp:226] da_conv5 needs backward computation.
I0302 01:27:40.404980 29253 net.cpp:226] da_relu4 needs backward computation.
I0302 01:27:40.404984 29253 net.cpp:226] bn4 needs backward computation.
I0302 01:27:40.404988 29253 net.cpp:226] da_conv4 needs backward computation.
I0302 01:27:40.404991 29253 net.cpp:226] da_relu3 needs backward computation.
I0302 01:27:40.404994 29253 net.cpp:226] bn3 needs backward computation.
I0302 01:27:40.404997 29253 net.cpp:226] da_conv3 needs backward computation.
I0302 01:27:40.405001 29253 net.cpp:226] da_relu2 needs backward computation.
I0302 01:27:40.405004 29253 net.cpp:226] bn2 needs backward computation.
I0302 01:27:40.405009 29253 net.cpp:226] da_conv2 needs backward computation.
I0302 01:27:40.405011 29253 net.cpp:226] da_relu1 needs backward computation.
I0302 01:27:40.405015 29253 net.cpp:226] bn1 needs backward computation.
I0302 01:27:40.405019 29253 net.cpp:226] da_conv1 needs backward computation.
I0302 01:27:40.405024 29253 net.cpp:228] data does not need backward computation.
I0302 01:27:40.405027 29253 net.cpp:270] This network produces output loss_cls
I0302 01:27:40.405045 29253 net.cpp:283] Network initialization done.
I0302 01:27:40.405107 29253 solver.cpp:60] Solver scaffolding done.
I0302 01:27:42.199970 29253 net.cpp:816] Ignoring source layer da_roi_pool5
I0302 01:27:42.199988 29253 net.cpp:816] Ignoring source layer da_fc6
I0302 01:27:42.199992 29253 net.cpp:816] Ignoring source layer bn6_2
I0302 01:27:42.199995 29253 net.cpp:816] Ignoring source layer da_relu6
I0302 01:27:42.199998 29253 net.cpp:816] Ignoring source layer da_drop6
I0302 01:27:42.200001 29253 net.cpp:816] Ignoring source layer da_fc7
I0302 01:27:42.200003 29253 net.cpp:816] Ignoring source layer bn7
I0302 01:27:42.200006 29253 net.cpp:816] Ignoring source layer da_relu7
I0302 01:27:42.200009 29253 net.cpp:816] Ignoring source layer da_drop7
I0302 01:27:42.200011 29253 net.cpp:816] Ignoring source layer bn7_da_drop7_0_split
I0302 01:27:42.200014 29253 net.cpp:816] Ignoring source layer da_cls_score
I0302 01:27:42.200016 29253 net.cpp:816] Ignoring source layer da_bbox_pred
I0302 01:27:42.200018 29253 net.cpp:816] Ignoring source layer da_loss_cls
I0302 01:27:42.200021 29253 net.cpp:816] Ignoring source layer da_loss_bbox
I0302 01:27:42.435448 29253 solver.cpp:237] Iteration 0, loss = 3.16784
I0302 01:27:42.435482 29253 solver.cpp:253]     Train net output #0: loss_cls = 3.16784 (* 1 = 3.16784 loss)
I0302 01:27:42.435492 29253 sgd_solver.cpp:106] Iteration 0, lr = 0.01
I0302 01:27:42.437265 29253 blocking_queue.cpp:50] Data layer prefetch queue empty
I0302 01:28:11.536866 29253 solver.cpp:237] Iteration 20, loss = 4.66288
I0302 01:28:11.536896 29253 solver.cpp:253]     Train net output #0: loss_cls = 4.66288 (* 1 = 4.66288 loss)
I0302 01:28:11.536905 29253 sgd_solver.cpp:106] Iteration 20, lr = 0.01
I0302 01:28:41.386526 29253 solver.cpp:237] Iteration 40, loss = 2.2039
I0302 01:28:41.386557 29253 solver.cpp:253]     Train net output #0: loss_cls = 2.2039 (* 1 = 2.2039 loss)
I0302 01:28:41.386565 29253 sgd_solver.cpp:106] Iteration 40, lr = 0.01
I0302 01:29:10.787991 29253 solver.cpp:237] Iteration 60, loss = 2.01626
I0302 01:29:10.788022 29253 solver.cpp:253]     Train net output #0: loss_cls = 2.01626 (* 1 = 2.01626 loss)
I0302 01:29:10.788029 29253 sgd_solver.cpp:106] Iteration 60, lr = 0.01
I0302 01:29:39.820272 29253 solver.cpp:237] Iteration 80, loss = 2.07329
I0302 01:29:39.820302 29253 solver.cpp:253]     Train net output #0: loss_cls = 2.07329 (* 1 = 2.07329 loss)
I0302 01:29:39.820312 29253 sgd_solver.cpp:106] Iteration 80, lr = 0.01
I0302 01:30:08.727838 29253 solver.cpp:237] Iteration 100, loss = 1.90712
I0302 01:30:08.727869 29253 solver.cpp:253]     Train net output #0: loss_cls = 1.90712 (* 1 = 1.90712 loss)
I0302 01:30:08.727877 29253 sgd_solver.cpp:106] Iteration 100, lr = 0.01
I0302 01:30:37.756769 29253 solver.cpp:237] Iteration 120, loss = 1.64091
I0302 01:30:37.756801 29253 solver.cpp:253]     Train net output #0: loss_cls = 1.64091 (* 1 = 1.64091 loss)
I0302 01:30:37.756809 29253 sgd_solver.cpp:106] Iteration 120, lr = 0.01
I0302 01:31:06.738243 29253 solver.cpp:237] Iteration 140, loss = 1.80336
I0302 01:31:06.738272 29253 solver.cpp:253]     Train net output #0: loss_cls = 1.80336 (* 1 = 1.80336 loss)
I0302 01:31:06.738281 29253 sgd_solver.cpp:106] Iteration 140, lr = 0.01
